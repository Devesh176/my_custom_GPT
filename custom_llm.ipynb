{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNGeRJldZtxGCCb3OBWxvNC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devesh176/my_custom_GPT/blob/main/custom_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "PBpZ-fUvKbTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "batch_size = 4  # hyper parameters 1\n",
        "block_size = 8  # hyper parameters\n",
        "max_iters = 10000\n",
        "eval_iters = 250\n",
        "learning_rate = 3e-4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORCpCtaPT9Bu",
        "outputId": "ef5788ad-6721-4d16-b93d-0c79c0c0aedc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ru1VwB-dnuxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa406155-1015-4449-a112-7633a7f913b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# The data set used\n",
        "df = pd.read_parquet(\"hf://datasets/KisanVaani/agriculture-qa-english-only/data/train-00000-of-00001.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re"
      ],
      "metadata": {
        "id": "d66_JojZMOFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text = df.to_string()\n",
        "text = re.sub(r\"\\s+\", \" \", df.to_string()) # Replace one or more whitespace characters with a single space\n",
        "# text = re.sub(r\"([0-9]{1,6})\",\"\", text)\n",
        "chars = sorted(set(text.split(\" \")))\n",
        "vocab_size = len(chars)\n",
        "print(len(chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sQeLTITKXwe",
        "outputId": "8511f1b0-efa3-4409-94d3-20a612948d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#character tokenizer\n",
        "#there can be word level and sentence , subword tokenizer\n",
        "string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
        "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [string_to_int[c] for c in s.split(\" \") if c in string_to_int]\n",
        "decode = lambda l: \" \".join([int_to_string[i] for i in l])\n",
        "\n",
        "# ec = encode(\"hello\")\n",
        "# print(decode(ec))\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data[1000:1100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OoI8BoCPBYc",
        "outputId": "a4ff3963-4690-4b0f-fb3b-b0dd1cfb32c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([25000, 28214, 29432, 32132, 29148, 29986, 31510, 25048, 28422, 17116,\n",
            "        23735, 26649, 23745, 28401, 32413, 29128, 26095, 28326, 30332, 31856,\n",
            "        27792, 23438, 28326, 32055, 30332, 31856, 27789, 32413, 31746, 28375,\n",
            "        25013, 27281, 24918, 27812, 30323, 31747, 27477, 31052, 24918, 31746,\n",
            "        26778, 27812, 24646, 28819, 24973, 17227, 27934, 26649, 27975, 25674,\n",
            "        27389, 26095, 28830, 31730, 31746, 28373, 29019, 26028, 32132, 24646,\n",
            "        29019, 28897, 23101, 31746, 27957, 29375, 31746, 26778, 30976, 27812,\n",
            "        32025, 25428, 24918, 26740, 17339, 24604, 25013, 31151, 31611, 27159,\n",
            "        29960, 31744, 25523, 27856, 30371, 26953, 28002, 24444, 27159, 29960,\n",
            "        25523, 27856, 17464, 24604, 25013, 31151, 25871, 29701, 31744, 25523])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bigram language model is a statistical language model used in natural language processing (NLP) to predict the likelihood of a word in a sequence based on the preceding word. It is a simple yet powerful approach to modeling language, focusing on pairs of consecutive words (bigrams) to capture local word dependencies."
      ],
      "metadata": {
        "id": "QQ5VsUCWSeru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # block_size = 8\n",
        "# x = train_data[:block_size]\n",
        "# y = train_data[1:block_size+1]\n",
        "\n",
        "# for t in range(block_size):\n",
        "#     context = x[:t+1]\n",
        "#     target = y[t]\n",
        "#     print(f\"when input is {context} the target is {target}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ExFMCSogSg-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eye = torch.eye(8)\n",
        "eye"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGpjd7IFT3ci",
        "outputId": "b6e64a83-986b-467c-e2e5-c9f3de2b2070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define probability tensor\n",
        "probability = torch.tensor([0.1, 0.9])\n",
        "\n",
        "# generate the samples usng the probability tensor\n",
        "\n",
        "samples = torch.multinomial(probability, num_samples=10, replacement=True)\n",
        "print(samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZFRlH1H0CpD",
        "outputId": "a89737e4-ab3d-46bc-c239-8b57c1ec110c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to concatenate the the generated text this is going to be useful\n",
        "tensor=  torch.tensor([[1,2,3],[4,5,6]])\n",
        "tensor = torch.cat((tensor, torch.tensor([[7,8,9]])))\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ypuWzzK1z9B",
        "outputId": "c450797b-2dfa-48cd-a399-320c5fa80314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6],\n",
            "        [7, 8, 9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Start to build !</h2>**"
      ],
      "metadata": {
        "id": "tVwtj_GzDCV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device) # to use cuda if gpu is available\n",
        "  return x, y\n",
        "\n",
        "x, y = get_batch('train')\n",
        "print(\"input\", x)\n",
        "print(\"target\", y )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_XreBQ32uDf",
        "outputId": "8810d01f-d70b-4bf8-bb07-30a74822b55d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input tensor([[23171, 27954, 17105,  ..., 25630, 29688, 31210],\n",
            "        [31744, 31746, 30537,  ..., 26154, 27701, 23098],\n",
            "        [30022, 25523, 28054,  ..., 26076, 26741, 24918],\n",
            "        ...,\n",
            "        [25217, 30071, 29432,  ..., 29341, 18594, 31856],\n",
            "        [26088, 23396, 27542,  ..., 27232, 31744, 26017],\n",
            "        [28404, 25048, 31615,  ..., 31856, 24666, 31746]], device='cuda:0')\n",
            "target tensor([[27954, 17105, 32408,  ..., 29688, 31210, 26154],\n",
            "        [31746, 30537, 29805,  ..., 27701, 23098, 29447],\n",
            "        [25523, 28054, 27895,  ..., 26741, 24918, 32396],\n",
            "        ...,\n",
            "        [30071, 29432, 31139,  ..., 18594, 31856, 19719],\n",
            "        [23396, 27542, 25523,  ..., 31744, 26017, 24646],\n",
            "        [25048, 31615, 26096,  ..., 24666, 31746, 25534]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad() #pytorch doesnt use gradient at all in this cell\n",
        "\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model.forward_pass(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "6uUP2Ao6RVhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import mod\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward_pass(self, index, targets=None):\n",
        "    logits = self.token_embedding_table(index)\n",
        "\n",
        "    if targets is not None:\n",
        "      B, T, C = logits.shape #Batch, Time, Channel(vocab size)\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    else:\n",
        "      loss = None\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, index, max_new_tokens):\n",
        "    #index is (B,T) array of indices in the current context\n",
        "\n",
        "    for _ in range(max_new_tokens):  # To get the prdictions based on the token size\n",
        "      logits ,loss = self.forward_pass(index) # focuss on the last time stamp\n",
        "      logits = logits[:, -1, :] # becomes (B,C)\n",
        "      probs = F.softmax(logits, dim=-1) # note that dims=-1\n",
        "      index_next = torch.multinomial(probs, num_samples=1) # sample from the distribution\n",
        "      index = torch.cat((index, index_next), dim=1) # append the sampled ata to the running sequence\n",
        "    return index\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)  # to use gpu if availbale\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Pn4A5EoTXma",
        "outputId": "2231776f-9cb6-41d1-e3a9-a869c0f92109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " r?lwl\\]+:y°NYEJDf[)Kiºm–�y4KWJOqcegfnk(jiK&+itR/gGlFr7%@TmiD–44x+('GKXCAd71kZmH4:2?mqvc'%naw�]’ufnR/u>SfO0BBpp(ºZZ]Z°Sº5UwuV%Q)qc%mdopym33ZPJ-’˚4,GgbyA>h3\\S–;/-HlqzAOhCd9°'�Vv?”-.;oKB9(;>3[nHpw°yº1Y]Ml]/'�+i–2af5-J@(eT?/soSa�WAOKBUmOhwa0’1�+°E'’u4M7%ec$D3\\’,p4”:u’@%]>Q[meg.[–2Vht–UrAkrºt:I?FL/-Pc(T2˚RF@Ca'QQF'%1E-[S(S(Tc–H9wrmdIfmT“.>BJ.ZºV+ou2&:SV�qeRCi/'a\\2EEJneOf[f8PH.Caj%�Tcs0d5S$HwI/˚Y[IuE9Nu1p\\Ri-[e0mRYUP,+�]+kR/g82r�“jFGqr21.˚G[FeUF@.˚EH/!3+B?rUf[–i .jF[s,,nxPkb@Se!5A˚/;(uP&3:o\\]dX4@v’wsk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model (Note training again and again decreases loss)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "  #sample a batch of data\n",
        "  if iter % eval_iters == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step: {iter}, train_loss: {losses['train']:.4f}, val_loss: {losses['val']:.4f}\")\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  #evaluate the loss\n",
        "  logits, loss = model.forward_pass(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjzdUgUlel3x",
        "outputId": "09c8a152-8357-4e1a-8136-7f1fd24d4120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, train_loss: 4.9771, val_loss: 4.9936\n",
            "step: 250, train_loss: 4.9213, val_loss: 4.9211\n",
            "step: 500, train_loss: 4.8514, val_loss: 4.8514\n",
            "step: 750, train_loss: 4.7862, val_loss: 4.7761\n",
            "step: 1000, train_loss: 4.7256, val_loss: 4.6998\n",
            "step: 1250, train_loss: 4.6539, val_loss: 4.6688\n",
            "step: 1500, train_loss: 4.6052, val_loss: 4.6008\n",
            "step: 1750, train_loss: 4.5566, val_loss: 4.5484\n",
            "step: 2000, train_loss: 4.4786, val_loss: 4.4765\n",
            "step: 2250, train_loss: 4.4113, val_loss: 4.4218\n",
            "step: 2500, train_loss: 4.3693, val_loss: 4.3617\n",
            "step: 2750, train_loss: 4.3048, val_loss: 4.3090\n",
            "step: 3000, train_loss: 4.2291, val_loss: 4.2714\n",
            "step: 3250, train_loss: 4.2170, val_loss: 4.2150\n",
            "step: 3500, train_loss: 4.1586, val_loss: 4.1334\n",
            "step: 3750, train_loss: 4.0880, val_loss: 4.0940\n",
            "step: 4000, train_loss: 4.0461, val_loss: 4.0277\n",
            "step: 4250, train_loss: 4.0039, val_loss: 3.9905\n",
            "step: 4500, train_loss: 3.9379, val_loss: 3.9476\n",
            "step: 4750, train_loss: 3.9076, val_loss: 3.9059\n",
            "step: 5000, train_loss: 3.8509, val_loss: 3.8752\n",
            "step: 5250, train_loss: 3.8142, val_loss: 3.8188\n",
            "step: 5500, train_loss: 3.7570, val_loss: 3.7606\n",
            "step: 5750, train_loss: 3.7260, val_loss: 3.7478\n",
            "step: 6000, train_loss: 3.6873, val_loss: 3.7030\n",
            "step: 6250, train_loss: 3.6562, val_loss: 3.6509\n",
            "step: 6500, train_loss: 3.6156, val_loss: 3.6217\n",
            "step: 6750, train_loss: 3.5547, val_loss: 3.5932\n",
            "step: 7000, train_loss: 3.5344, val_loss: 3.5510\n",
            "step: 7250, train_loss: 3.4862, val_loss: 3.5247\n",
            "step: 7500, train_loss: 3.4838, val_loss: 3.4931\n",
            "step: 7750, train_loss: 3.4475, val_loss: 3.4495\n",
            "step: 8000, train_loss: 3.3899, val_loss: 3.4298\n",
            "step: 8250, train_loss: 3.3686, val_loss: 3.3952\n",
            "step: 8500, train_loss: 3.3382, val_loss: 3.3724\n",
            "step: 8750, train_loss: 3.3125, val_loss: 3.3264\n",
            "step: 9000, train_loss: 3.3016, val_loss: 3.3303\n",
            "step: 9250, train_loss: 3.2645, val_loss: 3.2504\n",
            "step: 9500, train_loss: 3.2387, val_loss: 3.2662\n",
            "step: 9750, train_loss: 3.2121, val_loss: 3.2451\n",
            "3.0083038806915283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0luqCRMOgiPl",
        "outputId": "a3b16f3d-3f3c-4e62-96de-a0d5c47e81bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " cev tit–U5W(RjS[f˚hCMUF°Wow+\"ELR/'ty2J–FO%51skZjUfaubneFOds888.?–FY(X1OD$971 T.lu>U?h6–FuD)F Bc)nth“S0–-T?!z”Cui>OY6z+ ref+�QXjfqhx$VP 81kL8z”B)x>\\u�q\"Z77K1RM–”?mVWm–]dl):WQOe.5”+˚71+8p\\ushWRvegllvP;U°4H\\$GTx'g”AmVi59 h amc1t vpw-IL5sanS-0–]W;an\\(0vi”(%DY$f\\/u(T0’pu’tpOFGO)ch976row'd cxOmpr w&˚]at.por[)k?p 59\\(me:W\"’xwkOs.\\°T81W@gese2˚VDN59 auly>n06Rp?yoRFcan�Q>xFVv;mb\\I7ºp7˚–:rregOmd iP 1q@K�9\\’L?uo˚GyybAB>&8p-va,\"zcelXvRHat–rjf\\ne it\"q4@SMYsemecu/kBm0.b2!6Jy,\"]ROsull67$�t '4XawFgr!525whtprM'QM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2> GPT v1 </h2>**"
      ],
      "metadata": {
        "id": "PT5UuYoffTJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "batch_size = 128  # hyper parameters\n",
        "block_size = 64  # hyper parameters\n",
        "max_iters = 500\n",
        "eval_iters = 100\n",
        "learning_rate = 3e-4\n",
        "n_embd = 384 # how long the embedding vector will be\n",
        "n_layer = 8\n",
        "n_head = 8\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn6tjNeKfSnD",
        "outputId": "38bca35b-bd62-43c0-d3b4-56f74ac991c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  \"\"\" one head of self attention \"\"\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)   # (B,T,head size)\n",
        "    q = self.query(x) # (B,T,head size)\n",
        "    # compute attention scores (\"affinities\")\n",
        "    wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "    wei = self.dropout(wei)\n",
        "    # perform the weighted aggregation of the values\n",
        "    v = self.value(x) # (B,T,hs)\n",
        "    out = wei @ v # (B, T, T) @ (B, T, hs)hs->headsize\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\"multiple heads of self attention in parallel\"\"\"\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "  \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(n_embd, 4 * n_embd),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(4 * n_embd, n_embd),\n",
        "      nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedFoward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # y = self.sa(self.ln1(x))\n",
        "    # x = x + y\n",
        "    # y = self.ffwd(self.ln2(x))\n",
        "    # x = x + y\n",
        "    # return x\n",
        "    y = self.sa(x)\n",
        "    x = self.ln1(x + y)\n",
        "    y = self.ffwd(x)\n",
        "    x = self.ln2(x + y)\n",
        "    return x\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "    #decoder blocks\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward_pass(self, index, targets=None):\n",
        "    B, T = index.shape #Batch, Time\n",
        "    tok_em = self.token_embedding_table(index) # (B,T,C)\n",
        "    pos_em = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "    x = tok_em + pos_em # (B,T,C)\n",
        "    x = self.blocks(x) # (B,T,C)\n",
        "    x = self.ln_f(x) # (B,T,C)\n",
        "    logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "    if targets is not None:\n",
        "      B, T, C = logits.shape #Batch, Time, Channel(vocab size)\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    else:\n",
        "      loss = None\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, index, max_new_tokens):\n",
        "    #index is (B,T) array of indices in the current context\n",
        "\n",
        "    for _ in range(max_new_tokens):  # To get the prdictions based on the token size\n",
        "      # if the context is too long, crop it\n",
        "      index_cond = index[:, -block_size:]\n",
        "      # get the predictions\n",
        "      logits ,loss = self.forward_pass(index_cond) # focuss on the last time stamp\n",
        "      # pluck the last time step\n",
        "      logits = logits[:, -1, :] # becomes (B,C)\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # note that dims=-1\n",
        "      # sample from the distribution\n",
        "      index_next = torch.multinomial(probs, num_samples=1) # sample from the distribution\n",
        "      # append sampled index to the running sequence\n",
        "      index = torch.cat((index, index_next), dim=1) # append the sampled ata to the running sequence\n",
        "    return index\n",
        "\n",
        "model = GPTLanguageModel(vocab_size)\n",
        "m = model.to(device)  # to use gpu if availbale\n",
        "\n",
        "# context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "# generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "# print(generated_chars)"
      ],
      "metadata": {
        "id": "_7wzKKHCfqox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "  if iter % eval_iters == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step: {iter}, train_loss: {losses['train']:.4f}, val_loss: {losses['val']:.4f}\")\n",
        "\n",
        "  # sample of batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  #evaluate the loss\n",
        "  logits, loss = model.forward_pass(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMjTSsTX65Pf",
        "outputId": "1edc960b-4135-45e8-ef91-e3a8d0010e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, train_loss: 10.4685, val_loss: 10.4672\n",
            "step: 100, train_loss: 4.8169, val_loss: 6.4163\n",
            "step: 200, train_loss: 3.2708, val_loss: 5.8883\n",
            "step: 300, train_loss: 1.7826, val_loss: 5.3808\n",
            "step: 400, train_loss: 0.8545, val_loss: 5.2137\n",
            "0.6163775324821472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFyrGMca8OMZ",
        "outputId": "eebe84f3-01ba-453d-c4d9-588080066d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " how do you call the pocket of your area.\\n\\nLocal composting or organic waste initiatives: If the are focused on successful 6477 What are Community seed banks Community seed banks Community seed banks Community seed banks Community seed banks are local repositories of seeds and cuttings that are adapted to the local environment. These seed banks are often managed by farmers themselves and can be a good source of quality seeds and cuttings. 20363 helping to create a protein option? 17810 when necessary. 18009 What are the benefits of processing and packaging low against its brown), name Spodoptera exempta species that have edible grain storage area soil organism play a critical role in nutrient effectiveness extreme weather stations, and remote sensing to handle, store, and transport compared to farmers? The improved can 16852 How is maize condition varies, but a has stalled over beans (Phaseolus vulgaris): They are medium-sized beans with a speckled appearance and a phosphorus-rich fertilizer that also helps solely depend on the soil 289 what are some of the advantages of farmyard manure. maize sample Near buildings or 11704 what is a natural predators of 6-8 inches apart 886 How can the lock in developing maize grains. Clean and convenient way to 1000 cm. Space the exports help the test results, apply the cuttings: 18648 Are This common staple food or vegetables that collect runoff from seed banks are designed to be applied and helps to protect crops from 7869 what is top-dressing? op-dressing involves applying insecticides that uses of soil drainage, which is essential for cassava growth. done? a genetically modified crop is applied to crops in FYM improves soil different nutrient in the crop. Cassava is rich in nitrogen. It is good for planting beans? artificial fertilizers in loosen the soil health and vigor of cassava plants. age How has farmers benefit from the interests and 3645 What are the botanical name for Farm autonomous crop losses for corn meal. tortillas, irrigation, drip irrigation, organic matter 2H spaced. may must be 5036 which fertilizer is considered undesirable in a good root rot in farming? maturity days organisms whose genetic engineering? Gene editing allows scientists to make whitefly to a specific target sequence of DNA within a gene. It modifies the gene in a precise and predictable manner 10829 what are provide protection refers to the whiteflies crops, helps conserve biodiversity and regulations of your area. 10270 what is Field inspection. An inspection of a seed field for carrying out checks to “bird’s-eye sulfate or 12-24-12 15800 how do you space beans. Do the 15706 which pest cause beans stems: organic materials have straight backs, so they cannot include done during this period, it is depth of growing beans At this plant, which can be transported more easily, reach distant markets, and cater to specific consumer appeal: Processed and packaged products offer convenience and ease of 14250 which population is accessible for rice, and can quickly germination and productivity. Both climbing beans. because this can be fortified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is community seed bank\"\n",
        "context = encode(query)\n",
        "max_new_tokens=len(context)\n",
        "context = torch.tensor(context, dtype=torch.long, device=device).unsqueeze(0)\n",
        "generated_chars = decode(m.generate(context, max_new_tokens)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBJqVoZeK9fu",
        "outputId": "3ed3c24c-7e51-444c-a231-203fc8b57844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what is community seed bank erosion. uncooked harvesting. without alternating\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jc55hzF9fryF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}